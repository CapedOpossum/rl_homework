{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole Using Explicit Policy\n",
    "This notebook contains the code that implement solutions to the OpenAI Gym \"Cart Pole\" problem using explicit policies that do not use reinforcement learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Initialization\n",
    "This section contains the constants, variables, and functions used by all of the problem solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "LEFT_ACTION:int = 0\n",
    "RIGHT_ACTION:int = 1\n",
    "OBS_CART_POS_IDX:int = 0\n",
    "OBS_CART_VEL_IDX:int = 1\n",
    "OBS_POLE_POS_IDX:int = 2\n",
    "OBS_POLE_ANGVEL_IDX:int = 3\n",
    "MAX_STEP_COUNT:int = 200\n",
    "\n",
    "# Global Variables\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Function: Translate action to string\n",
    "def string_from_action(the_action):\n",
    "    if the_action == LEFT_ACTION:\n",
    "        return \"LEFT\"\n",
    "    else:\n",
    "        return \"RIGHT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy One: Counter Angular Velocity\n",
    "The policy implemented in this section consists of observing the angular velocity of the pole, and selecting the action that will counter (i.e., reduce) the observed angular velocity. Thus, if the angular velocity of the pole is observed to be in the counter-clockwise direction, the action will be to slide the cart left. Conversely, if the angular velocity of the pole is observed to be in the clockwise direction, the action will be to slide the cart right. The expectation is that as the pole begins to sway in a specific direction, the cart will move to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 160.000000\n",
      "Episode 2 Reward: 153.000000\n",
      "Episode 3 Reward: 200.000000\n",
      "Episode 4 Reward: 200.000000\n",
      "Episode 5 Reward: 200.000000\n",
      "Episode 6 Reward: 200.000000\n",
      "Episode 7 Reward: 200.000000\n",
      "Episode 8 Reward: 200.000000\n",
      "Episode 9 Reward: 200.000000\n",
      "Episode 10 Reward: 200.000000\n",
      "Episode 11 Reward: 136.000000\n",
      "Episode 12 Reward: 200.000000\n",
      "Episode 13 Reward: 138.000000\n",
      "Episode 14 Reward: 200.000000\n",
      "Episode 15 Reward: 200.000000\n",
      "Episode 16 Reward: 200.000000\n",
      "Episode 17 Reward: 200.000000\n",
      "Episode 18 Reward: 158.000000\n",
      "Episode 19 Reward: 200.000000\n",
      "Episode 20 Reward: 146.000000\n",
      "Policy One Average reward: 184.550000\n"
     ]
    }
   ],
   "source": [
    "def counter_angular_velocity():\n",
    "    with open(\"./02_cart_pole_counter_angvel.csv\", \"w\") as trace_file:\n",
    "        print(\"Episode,Angular Vel,Action\", file=trace_file)\n",
    "        total_reward:float = 0.0\n",
    "        for episode_idx in range(20):\n",
    "            observation = env.reset()\n",
    "            action:int = LEFT_ACTION if observation[OBS_POLE_ANGVEL_IDX] < 0.0 else RIGHT_ACTION\n",
    "            episode_reward:float = 0.0\n",
    "            done:bool = False\n",
    "            step_count:int = 0\n",
    "            while not done and (step_count < MAX_STEP_COUNT):\n",
    "                env.render()\n",
    "                angvel:float = observation[OBS_POLE_ANGVEL_IDX]\n",
    "                print('{:d},{:f},{:s}'.format(episode_idx + 1, angvel, string_from_action(action)), file=trace_file)\n",
    "                if angvel < 0.0:\n",
    "                    action = LEFT_ACTION\n",
    "                else:\n",
    "                    action = RIGHT_ACTION\n",
    "                observation, step_reward, done, info = env.step(action)\n",
    "                episode_reward += step_reward\n",
    "                step_count += 1\n",
    "            print('Episode {:d} Reward: {:f}'.format(episode_idx + 1, episode_reward))\n",
    "            total_reward += episode_reward\n",
    "    print('Policy One Average reward: {:f}'.format(total_reward / (episode_idx + 1)))\n",
    "    env.close()\n",
    "\n",
    "counter_angular_velocity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Two: Pole Angular Position\n",
    "The policy implemented in this section consists of the angle of the pole. If we consider the pole's angular range of motion as a circle with angle $0$ pointing up, whenever the angle of the pole tilts into the semicircle area defined by the angle range $[\\pi,2\\pi]$, the cart will push to the left. Whenver the angle of the pole tilts into the semicircle area defined by the angle range $[0,\\pi]$, the cart will push to the right. The expectation is that the pole's angular position will be constrained to a narrow band, thus surviving the requisite 200 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 46.000000\n",
      "Episode 2 Reward: 38.000000\n",
      "Episode 3 Reward: 41.000000\n",
      "Episode 4 Reward: 45.000000\n",
      "Episode 5 Reward: 39.000000\n",
      "Episode 6 Reward: 51.000000\n",
      "Episode 7 Reward: 46.000000\n",
      "Episode 8 Reward: 45.000000\n",
      "Episode 9 Reward: 36.000000\n",
      "Episode 10 Reward: 34.000000\n",
      "Episode 11 Reward: 52.000000\n",
      "Episode 12 Reward: 38.000000\n",
      "Episode 13 Reward: 46.000000\n",
      "Episode 14 Reward: 36.000000\n",
      "Episode 15 Reward: 39.000000\n",
      "Episode 16 Reward: 49.000000\n",
      "Episode 17 Reward: 42.000000\n",
      "Episode 18 Reward: 35.000000\n",
      "Episode 19 Reward: 52.000000\n",
      "Episode 20 Reward: 51.000000\n",
      "Policy Two Average reward: 43.050000\n"
     ]
    }
   ],
   "source": [
    "def pole_angular_position():\n",
    "    with open(\"./02_cart_pole_angular_pos.csv\", \"w\") as trace_file:\n",
    "        print(\"Episode,Pole Angle,Angular Vel,Action\", file=trace_file)\n",
    "        total_reward:float = 0.0\n",
    "        for episode_idx in range(20):\n",
    "            observation = env.reset()\n",
    "            action:int = LEFT_ACTION\n",
    "            if observation[OBS_POLE_POS_IDX] > 0.0:\n",
    "                action = RIGHT_ACTION\n",
    "            episode_reward:float = 0.0\n",
    "            done:bool = False\n",
    "            step_count:int = 0\n",
    "            while not done and (step_count < MAX_STEP_COUNT):\n",
    "                env.render()\n",
    "                pole_ang_pos:float = observation[OBS_POLE_POS_IDX]\n",
    "                if pole_ang_pos < 0.0:\n",
    "                    action = LEFT_ACTION\n",
    "                else:\n",
    "                    action = RIGHT_ACTION\n",
    "                print(\n",
    "                    '{:d},{:f},{:f},{:s}'\n",
    "                    .format(\n",
    "                        episode_idx + 1,\n",
    "                        pole_ang_pos,\n",
    "                        observation[OBS_POLE_ANGVEL_IDX],\n",
    "                        string_from_action(action)\n",
    "                    ),\n",
    "                    file=trace_file\n",
    "                )\n",
    "                observation, step_reward, done, info = env.step(action)\n",
    "                episode_reward += step_reward\n",
    "                step_count += 1\n",
    "            print('Episode {:d} Reward: {:f}'.format(episode_idx + 1, episode_reward))\n",
    "            total_reward += episode_reward\n",
    "    print('Policy Two Average reward: {:f}'.format(total_reward / (episode_idx + 1)))\n",
    "    env.close()\n",
    "\n",
    "pole_angular_position()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Three: Pole Angular Velocity P-Loop\n",
    "The policy implemented in this section consists of implementing a simplified proportional control feedback loop on the pole's angular velocity. The P-Loop will attempt to keep the pole's angular velocity at $0.0$, calculating the error between the actual velocity observed and the target velocity ($0.0$ in our case). The P-Loop is simplified because, although the signal produced will be proportional, the cart's command input is discrete. The expectation is that the P-Loop logic will do its best at maintaining angular velocity near $0.0$ for the requisite 200 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 150.000000\n",
      "Episode 2 Reward: 200.000000\n",
      "Episode 3 Reward: 169.000000\n",
      "Episode 4 Reward: 200.000000\n",
      "Episode 5 Reward: 153.000000\n",
      "Episode 6 Reward: 200.000000\n",
      "Episode 7 Reward: 200.000000\n",
      "Episode 8 Reward: 144.000000\n",
      "Episode 9 Reward: 186.000000\n",
      "Episode 10 Reward: 140.000000\n",
      "Episode 11 Reward: 200.000000\n",
      "Episode 12 Reward: 200.000000\n",
      "Episode 13 Reward: 168.000000\n",
      "Episode 14 Reward: 148.000000\n",
      "Episode 15 Reward: 196.000000\n",
      "Episode 16 Reward: 200.000000\n",
      "Episode 17 Reward: 197.000000\n",
      "Episode 18 Reward: 173.000000\n",
      "Episode 19 Reward: 200.000000\n",
      "Episode 20 Reward: 182.000000\n",
      "Policy Three Average reward: 180.300000\n"
     ]
    }
   ],
   "source": [
    "def p_loop_calc(actual, target, current_action):\n",
    "    K_P:float = 0.75\n",
    "    error:float = target - actual\n",
    "    output:float = error * K_P\n",
    "    # Dead zone\n",
    "    if math.fabs(output) < 0.001:\n",
    "        return current_action, error\n",
    "    # Action based on output\n",
    "    if output > 0.0:\n",
    "        return LEFT_ACTION, error\n",
    "    else:\n",
    "        return RIGHT_ACTION, error\n",
    "\n",
    "def pole_ang_vel_p_loop():\n",
    "    with open(\"./02_cart_pole_angvel_p_loop.csv\", \"w\") as trace_file:\n",
    "        print(\"Episode,Angular Vel,Error,Action\", file=trace_file)\n",
    "        total_reward:float = 0.0\n",
    "        for episode_idx in range(20):\n",
    "            observation = env.reset()\n",
    "            action, _ = p_loop_calc(observation[OBS_POLE_ANGVEL_IDX], 0.0, LEFT_ACTION)\n",
    "            episode_reward:float = 0.0\n",
    "            done:bool = False\n",
    "            step_count:int = 0\n",
    "            p_loop_err:float = 0.0\n",
    "            while not done and (step_count < MAX_STEP_COUNT):\n",
    "                env.render()\n",
    "                pole_ang_vel:float = observation[OBS_POLE_ANGVEL_IDX]\n",
    "                action, p_loop_err = p_loop_calc(pole_ang_vel, 0.0, action)\n",
    "                print(\n",
    "                    '{:d},{:f},{:f},{:s}'\n",
    "                    .format(\n",
    "                        episode_idx + 1,\n",
    "                        pole_ang_vel,\n",
    "                        p_loop_err,\n",
    "                        string_from_action(action)\n",
    "                    ),\n",
    "                    file=trace_file\n",
    "                )\n",
    "                observation, step_reward, done, info = env.step(action)\n",
    "                episode_reward += step_reward\n",
    "                step_count += 1\n",
    "            print('Episode {:d} Reward: {:f}'.format(episode_idx + 1, episode_reward))\n",
    "            total_reward += episode_reward\n",
    "    print('Policy Three Average reward: {:f}'.format(total_reward / (episode_idx + 1)))\n",
    "    env.close()\n",
    "\n",
    "pole_ang_vel_p_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
