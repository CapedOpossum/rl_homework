{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Using Explicit Policy\n",
    "\n",
    "The contents of this notebook implement several solutions to the OpenAI Gym \"Mountain Car\" problem using explicit, hard-coded policies that do not utilize reinforcement learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Setup\n",
    "This section contains the common setup code for all the policy implementations, as well as common utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "REVERSE_ACTION:int = 2\n",
    "FORWARD_ACTION:int = 0\n",
    "OBS_POS_IDX:int = 0\n",
    "OBS_VEL_IDX:int = 1\n",
    "\n",
    "# Global Variables\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Function: Toggle between forward and reverse actions\n",
    "def toggle_action(current_action):\n",
    "    if current_action == FORWARD_ACTION:\n",
    "        return REVERSE_ACTION\n",
    "    else:\n",
    "        return FORWARD_ACTION\n",
    "\n",
    "def string_from_action(the_action):\n",
    "    if the_action == FORWARD_ACTION:\n",
    "        return 'FORWARD'\n",
    "    else:\n",
    "        return 'BACKWARD'\n",
    "\n",
    "def string_from_bool(the_bool, true_string='True', false_string='False'):\n",
    "    if the_bool:\n",
    "        return true_string\n",
    "    else:\n",
    "        return false_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy One: Rapid Switch Back and Forth\n",
    "The policy implemented in this section will simply engage motion in the reverse direction up until it detects that cart speed is slower than 10<sup>-3</sup>. At that point, the policy will engage motion in the forward direction, and the process will repeat until the mountain car reaches the goal. The expectation of this policy is that the rapid back and forth motion will eventually let the cart build up momentum, helping it reach its goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: -117.000000\n",
      "Episode 2 Reward: -88.000000\n",
      "Episode 3 Reward: -116.000000\n",
      "Episode 4 Reward: -120.000000\n",
      "Episode 5 Reward: -116.000000\n",
      "Episode 6 Reward: -85.000000\n",
      "Episode 7 Reward: -109.000000\n",
      "Episode 8 Reward: -89.000000\n",
      "Episode 9 Reward: -103.000000\n",
      "Episode 10 Reward: -116.000000\n",
      "Episode 11 Reward: -115.000000\n",
      "Episode 12 Reward: -160.000000\n",
      "Episode 13 Reward: -114.000000\n",
      "Episode 14 Reward: -177.000000\n",
      "Episode 15 Reward: -122.000000\n",
      "Episode 16 Reward: -88.000000\n",
      "Episode 17 Reward: -87.000000\n",
      "Episode 18 Reward: -87.000000\n",
      "Episode 19 Reward: -116.000000\n",
      "Episode 20 Reward: -122.000000\n",
      "Policy One Average reward -112.350000\n"
     ]
    }
   ],
   "source": [
    "# Function: Rapid switch back and forth based on speed\n",
    "def rapid_speed_based_switch():\n",
    "    with open(\"./01_mountain_car_rapid_speed_switch.csv\", \"w\") as trace_file:\n",
    "        print(\"Episode,Speed,Action\", file=trace_file)\n",
    "        total_reward:float = 0.0\n",
    "        for episode_idx in range(20):\n",
    "            observation = env.reset()\n",
    "            action:int = REVERSE_ACTION\n",
    "            episode_reward:float = 0.0\n",
    "            done:bool = False\n",
    "            speed:float = 0.0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                speed = math.fabs(observation[OBS_VEL_IDX])\n",
    "                print(\"{:d},{:f},{:s}\".format(episode_idx + 1, speed, string_from_action(action)), file=trace_file)\n",
    "                if speed <= 1e-3:\n",
    "                    action = toggle_action(action)\n",
    "                observation, step_reward, done, info = env.step(action)\n",
    "                episode_reward += step_reward\n",
    "            print('Episode {:d} Reward: {:f}'.format(episode_idx + 1, episode_reward))\n",
    "            total_reward += episode_reward\n",
    "\n",
    "    print('Policy One Average reward {:f}'.format(total_reward / (episode_idx + 1)))\n",
    "    env.close()\n",
    "\n",
    "rapid_speed_based_switch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Two: Acceleration-Based Switching\n",
    "The policy implemented in this section is an enhancement over the policy from the previous section in that it measures acceleration instead of speed. This policy does not discard the direction component of the vector, like the previous one did. The policy starts by accelerating in the initial direction of motion, as indicated by the velocity component of the initial observation. Once the logic detects that acceleration has fallen below 10<sup>-3</sup>, it will change direction. The policy will then wait to observe an increase in acceleration before even considering an action switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: -118.000000\n",
      "Episode 2 Reward: -119.000000\n",
      "Episode 3 Reward: -122.000000\n",
      "Episode 4 Reward: -118.000000\n",
      "Episode 5 Reward: -113.000000\n",
      "Episode 6 Reward: -119.000000\n",
      "Episode 7 Reward: -115.000000\n",
      "Episode 8 Reward: -113.000000\n",
      "Episode 9 Reward: -114.000000\n",
      "Episode 10 Reward: -122.000000\n",
      "Episode 11 Reward: -119.000000\n",
      "Episode 12 Reward: -118.000000\n",
      "Episode 13 Reward: -113.000000\n",
      "Episode 14 Reward: -117.000000\n",
      "Episode 15 Reward: -121.000000\n",
      "Episode 16 Reward: -118.000000\n",
      "Episode 17 Reward: -113.000000\n",
      "Episode 18 Reward: -113.000000\n",
      "Episode 19 Reward: -117.000000\n",
      "Episode 20 Reward: -118.000000\n",
      "Policy Two Average reward -117.000000\n"
     ]
    }
   ],
   "source": [
    "# Function: Acceleration based switching\n",
    "def accel_based_switch():\n",
    "    with open(\"./01_mountain_car_accel_switch.csv\", \"w\") as trace_file:\n",
    "        print('Episode,Velocity,Accel,SwOK,Action', file=trace_file)\n",
    "        total_reward:float = 0.0\n",
    "        for episode_idx in range(20):\n",
    "            observation = env.reset()\n",
    "            action:int = REVERSE_ACTION if observation[OBS_VEL_IDX] < 0 else FORWARD_ACTION\n",
    "            episode_reward:float = 0.0\n",
    "            done:bool = False\n",
    "            accel:float = 0.0\n",
    "            last_vel:float = 0.0\n",
    "            ok_to_switch:bool = True\n",
    "            while not done:\n",
    "                print(\n",
    "                    '{:d},{:f},{:f},{:s},{:s}'\n",
    "                    .format(\n",
    "                        episode_idx + 1,\n",
    "                        observation[OBS_VEL_IDX],\n",
    "                        accel,\n",
    "                        string_from_bool(\n",
    "                            ok_to_switch,\n",
    "                            true_string='YES',\n",
    "                            false_string='NO'\n",
    "                        ),\n",
    "                        string_from_action(action)\n",
    "                    ),\n",
    "                    file=trace_file\n",
    "                )\n",
    "                env.render()\n",
    "                accel = math.fabs(observation[OBS_VEL_IDX]) - math.fabs(last_vel)\n",
    "                if ok_to_switch and accel < 1e-3:\n",
    "                    action = toggle_action(action)\n",
    "                    ok_to_switch = False\n",
    "                elif not ok_to_switch and accel > 1e-3:\n",
    "                    ok_to_switch = True\n",
    "                observation, step_reward, done, info = env.step(action)\n",
    "                episode_reward += step_reward\n",
    "            print('Episode {:d} Reward: {:f}'.format(episode_idx + 1, episode_reward))\n",
    "            total_reward += episode_reward\n",
    "    print('Policy Two Average reward {:f}'.format(total_reward / (episode_idx + 1)))\n",
    "    env.close()\n",
    "\n",
    "accel_based_switch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
