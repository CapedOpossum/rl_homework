{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Learned in Class\n",
    "The contents of this notebook present a quick overview of all the algorithms used during the Advanced Artificial Intelligence class (CAP 5636) in order to both learn and implement intelligent agents.\n",
    "\n",
    "## Uninformed Search\n",
    "As a prelude to solving problems using artificial intelligence techniques, we first covered three (3) different search mechanisms that, although useful, were considered uninformed. These techniques yielded results, albeit in an inefficient manner. The primary purpose for the following algorithms is to find a specific node in a tree structure. They only differ on the approach they use to find the aforementioned node.\n",
    "\n",
    "### Depth-First Search\n",
    "The \"Depth-Based\" tree search algorithm executes the search in the tree structure by recursively expanding the children of a particular node. The search iterations of this algorithm evaluate the \"current\" node for equivalence with the goal (at first, the \"current\" node is the tree root). Should the \"current\" node be equivalent with the goal, the algorithm ends. Failing to find equivalence with the goal, the iteration expand's the \"current\" node's children, and selects one of the expanded node's children as the \"current\" node, executing another search iteration.\n",
    "\n",
    "### Breadth-First Search\n",
    "The \"Breatdh-First\" tree search algorithm executes the search in the tree structure by iterating over sibling nodes prior to descending deeper into the three. The search iterations of this algorithm evaluate the \"current\" node for equivalence with teh goal (at first, the \"current\" node is the tree root). Should the \"current\" node be equivalent with the goal, the algorithm ends. Failing to find equivalence with the goal, the iteration visits the \"current\" node's siblings, making the next sibling the \"current\" node. Should no more sibling nodes remain, the search iteration selects the next sibling with children nodes, expands the children nodes, and selects one of the children nodes as the \"current\" node.\n",
    "\n",
    "### Uniform Cost Search\n",
    "The \"Uniform Cost\" tree search algorithm is somewhat based on \"Breadth-First\" search, except each transition between nodes is assigned a cost. After performing a search over all siblings at a tree level, the algorithm expands the children at that level in an ascending cost fashion, where \"Breath-First\" may follow a naive left-to-right approach.\n",
    "\n",
    "## Informed Search\n",
    "An evolution of uninformed search are the collection of algorithms that are somewhat \"smarter\" when they execute the search. These \"smarts\" take on the form of a heuristic value assigned to each node. This heuristic value is meant to provide an indication of how close a particular node is to the goal, not unlike the \"cold and hot\" game played by children.\n",
    "\n",
    "### Greedy Search\n",
    "In Greedy Search, the tree nodes are traversed based exclusively on the heurisitic value of each node, with the best (usually lowest) values being searched first, without regard to breadth or depth.\n",
    "\n",
    "### A* Search\n",
    "A* search combines the features of Uniform Cost Search and Greedy Search in order to traverse the tree. Node selection for tree traversal is based on the cost assigned per traversal arc (usually called the \"backward\" cost), combined with the heuristic value of each node (usually called the \"forward\" cost).\n",
    "\n",
    "## Adversarial Search\n",
    "The search algorithms presented up to this point merely focused on finding a goal in a node graph, assuming no adversary is present. Building off of these search techniques, algorithms that account for adversaries are known as \"Adversarial Search\" algorithms.\n",
    "\n",
    "### Minimax\n",
    "The Minimax Adversarial Search algorithm assumes that both adversaries are \"playing optimally\". The algorithm manifests as a tree search where the path to a terminal state is sought by evaluating nodes based on the goals of the adversaries. An adversary may wish to \"maximize\" the overall reward of a game (the \"max\" adversary) while other adversaries may wish to \"minimize\" the overall reward of the game (the \"min\" adversary).\n",
    "\n",
    "### $\\alpha$ $\\beta$ Pruning in Minimax\n",
    "Given the large amount of states that even a simple adversarial game may produce, an exhaustive Minimax Search is almost always unfeasible. In order to mitigate the impact of large state spaces, $\\alpha$ $\\beta$ pruning provides a way for entire branches of the search tree to be eliminated from consideration as early as possible. The pruning algorithm leverages the stipulated goals of the adversaries (\"maximizing\" vs. \"minimizing\") in order to disqualify state subspaces based on previously-observed max/min values (the $\\alpha$ and $\\beta$).\n",
    "\n",
    "### Expectimax Search\n",
    "The Expectimax Adversarial Search algorithm is a modification of the Minimax Search algorithm in that the stipulated goals of the adversaries are slightly altered. The algorithm still assumes there's an adversary attempting to maximize overall value, but it does away with the notion that the adversaries attempting to minimize overall value are acting optimally. Instead, Expectimax assumes the minimizing adversaries act based on a probability distribution over all of their possible actions. Minimizing adversaries select actions at random based on the aforementioned probability distribution.\n",
    "\n",
    "## Model-Based Reinforcement Learning\n",
    "During the course, we began our studies of Reinforcement Learning by understanding Markov Decision Processes (MDP). MDPs model an intelligent agent's interaction with the environment by examining states, actions, and rewards. MDPs were useful to understand concepts like values and policies derived from an environment. In Model-Based Reinforcement Learning, the goal is to basically derive a policy (i.e., what the intelligent agent should do) by attempting to discover MDP parameters (states, action probability distributions, and rewards) off of either prior or active learning. The class did not delve deep into what it labeled as Model-Based Reinforcement Learning, instead choosing to focus on Model-Free Reinforcement Learning.\n",
    "\n",
    "## Model-Free Reinforcement Learning\n",
    "In Model-Free Reinforcement Learning (RL), although MDP mechanics are employed, the inner parameters that define an MDP are not necessary to derive policies for an intelligent agent. Instead, Model-Free RL attempts to approximate the result of MDP application, and use these approximations to derive the policy the intelligent agent should apply.\n",
    "\n",
    "### Direct Evaluation\n",
    "In direct evaluation, an intelligent agent acts based on an initial policy (usually homogeneous), averaging the reward obtained at each individual state. Although easy to understand, this approach does not take into account the possible reward of adjacent states, and thus may eventually lead to a sub-optimal policy.\n",
    "\n",
    "### Temporal Difference Learning\n",
    "Temporal Difference Learning is a variation on Direct Evaluation in that it attempts to take into account the rewards of future states. The values of each state are updated after every observed iteration using something similar to a Bellman Value Update.\n",
    "\n",
    "### Q-Learning\n",
    "Q-Learning does not focus on learning singular values for each state and deriving a policy off of that, but instead in attempting to learn the \"Quality\" of each state transition for any given state. In Q-Learning, as the intelligent agent explores the environment, it updates these \"Quality\" values based on actions performed and rewards obtained. After enough iterations, these \"Quality\" values are used to derive an optimal policy for the agent to implement.\n",
    "\n",
    "### Cross-Entropy Method\n",
    "In Cross-Entropy Reinforcement Learning, instead of relying on \"Quality\" values like Q-Learning, the agent relies on learning from a set of state and action pairs that in past iterations yielded the best rewards. By electing to train on actions taken at specific states during the best performing episodes, the algorithm endeavours to eventually arrive at an optimal policy via a process that draws close parallels with \"natural selection.\" This algorithm, however, does not exhibit the stability and consistency observed in other RL methods, like Q-Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
