{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Using Cross-Entropy Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rnieves/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Create the Mountain Car game environment\n",
    "env = gym.make('MountainCar-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters ##\n",
    "\n",
    "There are several changes done here as compared with the version written for the \"CartPole\" problem:\n",
    "\n",
    "* The `state_size` and `action_size` variables were initialized using information provided by the OpenAI Gym environment.\n",
    "* The batch size used in the cross-entropy method was raised from `25` to `50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "hidden_layer_size = 128\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "max_episodes = 100\n",
    "\n",
    "max_steps = 200\n",
    "percentile = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, \n",
    "                 state_size = state_size, \n",
    "                 action_size = action_size, \n",
    "                 hidden_layer_size = hidden_layer_size,\n",
    "                 learning_rate = learning_rate, \n",
    "                 name = 'net'):\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "        \n",
    "            ### Prediction part\n",
    "        \n",
    "            # Input layer, state s is input\n",
    "            self.states = tf.placeholder(\n",
    "                tf.float32, \n",
    "                [None, state_size])\n",
    "            \n",
    "            # Hidden layer, ReLU activation\n",
    "            self.hidden_layer = tf.contrib.layers.fully_connected(\n",
    "                self.states, \n",
    "                hidden_layer_size)\n",
    "            \n",
    "            # Hidden layer, linear activation, logits\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                self.hidden_layer, \n",
    "                action_size,\n",
    "                activation_fn = None)\n",
    "            \n",
    "            # Output layer, softmax activation yields probability distribution for actions\n",
    "            self.probabilities = tf.nn.softmax(self.logits)\n",
    "    \n",
    "            ### Training part \n",
    "    \n",
    "            # Action a\n",
    "            self.actions = tf.placeholder(\n",
    "                tf.int32, \n",
    "                [None])\n",
    "            \n",
    "            # One-hot encoded action a \n",
    "            #\n",
    "            # encoded_action_vector = [1, 0] if action a = 0\n",
    "            # encoded_action_vector = [0, 1] if action a = 1\n",
    "            self.one_hot_actions = tf.one_hot(\n",
    "                self.actions, \n",
    "                action_size)\n",
    "\n",
    "            # cross entropy\n",
    "            self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits = self.logits, \n",
    "                labels = self.one_hot_actions)\n",
    "            \n",
    "            # cost\n",
    "            self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "            \n",
    "            # Optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "            \n",
    "    # get action chosen according to current probabilistic policy\n",
    "    def get_action(self, state):\n",
    "        feed_dict = { self.states : np.array([state]) } \n",
    "        probabilities = sess.run(self.probabilities, feed_dict = feed_dict)\n",
    "        \n",
    "        return np.random.choice(action_size, p=probabilities[0])\n",
    "    \n",
    "    # train based on batch\n",
    "    def train(self, batch):\n",
    "        states, actions = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.states : states,\n",
    "            self.actions : actions\n",
    "        }\n",
    "        \n",
    "        sess.run(self.optimizer, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Modification\n",
    "The default reward signal from the \"Mountain Car\" scenario is too sparse for it to be useful when implementing a solution involving a fully-connected neural network that uses a cross-entropy loss function. Thus, this solution uses a modified reward formula in an attempt to train the neural network to successfully achieve victory. The reward function modification went through a non-trivial evolution, with many schemes being rejected for failing to converge toward a winning solution after a reasonable number of training episodes.\n",
    "\n",
    "In all of the modified reward functions attempts, a single time step reward of $1,000$ would be awarded if the car reached the flag, perceived by the episode being done while more time steps remained in the episode. This single step reward would let the training algorithm detect a victory condition.\n",
    "\n",
    "### Attempt 1: Biased Position\n",
    "The Mountain Car scenario state representation consists of two (2) real numbers: car position and car velocity. Car position seems to be expressed in terms of the car position with respect to a flat X axis located laid horizontally across the scenario environment. Based on experimentation, the effective range of this X axis is $[-1.2, 0.5]$. Such a range exhibits a median value of $-0.85$, and it would be reasonable to assume this coordinate points to the lowest point in the environment's valley. Further observations, however, reveal that the X coordinate for the valley's lowest point is actually $-0.5$. Using this information, the first attempt at reward modification was to simply return the car's X-coordinate value, biased by $0.5$.\n",
    "\n",
    "$$ R(s) = s_{pos} + 0.5 $$\n",
    "\n",
    "This reward function exhibited improvement through the first $10,000$ training episodes or so, as observed via test episodes run on the model after every $100$ training episodes. However, after another $10,000$ training episodes, the model lingered around the bottom half of the hill leading to the flag, failing to leverage the far side of the valley for momentum.\n",
    "\n",
    "### Attempt 2: ReLU'd Biased Position\n",
    "Continuing the use of the mountain car's X-coordiate biased by $0.5$, the next reward function attempt consisted in doing away with a negative reward (i.e., a penalty) whenever the car crossed over into the far side of the valley. Instead, in the far side of the valley the scenario would not offer a reward at all. In essence the Biased Position reward function would be put through a Rectifying Linear Unit (ReLU). The hope would be that the lack of a penalty would at least remove the apparent prohibition on the use of the far side slope.\n",
    "\n",
    "$$ R(s) = \\max(0.0, (s_{pos} + 0.5)) $$\n",
    "\n",
    "Unfortunately, this modified reward function exhibited the same behavior as the original Biased Position: the car would not venture into the far side of the valley in order to gain momentum.\n",
    "\n",
    "### Attempt 3: Squared, ReLU'd Biased Position\n",
    "The next step involved the incentivization of gaining \"altitude\" on the slope closest to the flag, by deriving a reward that would grow in a non-linear fashion at higher positions in the near slope. In the far side of the valley, the reward would still be clamped at $0.0$. The intent driving this approach was to incentivize the model to reach higher positions in the slope nearest to the flag. The non-linear function used in this attempt was simply the squaring of the rectified X coordinate.\n",
    "\n",
    "$$ R(s) = (\\max(0.0, (s_{pos} + 0.5)))^2 $$\n",
    "\n",
    "Unfortunately, since the biased X coordinate range between the lowest point in the valley and the flag range between $[0.0, 1.0]$, the function's slope in this range is \"flatter\" than the linear range by itself. Only after crossing the $1.0$ value would a the square function being to exhibit a more pronounced slope compared to a linear function. Thus, the model failed to climb very high up the slope nearest to the flag.\n",
    "\n",
    "### Attempt 4: ReLU'd, Sinusoid Based on Biased Position\n",
    "The next reward function evolution attempted to leverage the fact that the valley \"shape\" closely resembled a sinusoid. Perhaps, if the biased X coordinate was used to derive the corresponding result of a sinusoidal function, and that result was used as the basis for the reward, it would provide a stronger incentive to \"climb the hill\", since the result of this function would grow at a faster clip than the X coordinate alone. The range of rewards in the far side of the hill would remain ReLU'd at $0.0$. The rewards in the near side of the valley would still be mapped to the range $[0.0, 1.0]$, but in a non-linear fashion as defined by $[\\sin(0.0), \\sin(\\pi/2)]$. Thus, the biased X coordinate $s_{bpos} \\in [0.0, 1.0]$ needs to be mapped to the range $\\theta \\in [0.0, \\pi/2]$ prior to being fed to the $\\sin()$ function.\n",
    "\n",
    "$$ \\frac{s_{bpos}}{1.0 - 0.0} = \\frac{\\theta}{\\pi/2 - 0.0} \\\\\n",
    "s_{bpos} = \\frac{\\theta}{\\pi/2} \\\\\n",
    "\\theta = \\frac{\\pi s_{bpos}}{2} \\\\\n",
    "R(s) = \\max(0.0, \\sin(\\theta))\n",
    "$$\n",
    "\n",
    "After this attempt failed, it became obvious that, regardless of the non-linear rewards in the near side of the valley, refraining from providing any reward in the far side of the valley was detrimental to the successful training of the model.\n",
    "\n",
    "### Attempt 5: \"Altitude\" Reward\n",
    "Starting from the ReLU'd sinusoid attempt, if the step reward was at least based on \"how high\" the model drove the cart, in either direction, perhaps the model would eventually learn to \"aim for the fences\" and, consequently, reach the flag. Retaining the previous mapping of the biased X coordinate $s_{bpos} \\in [0.0, 1.0]$ to the range $\\theta \\in [0.0, \\pi/2]$, a new mapping for the far side of the valley could be derived such that $s_{bpos} \\in [-0.7, 0.0]$ maps to $\\theta_f \\in [\\pi/2, 0.0]$.\n",
    "\n",
    "$$ \\frac{s_{bpos}}{0.0 - -0.7} = \\frac{\\theta_f}{\\pi/2 - 0.0} \\\\\n",
    "\\frac{s_{bpos}}{0.7} = \\frac{\\theta_f}{\\pi/2} \\\\\n",
    "\\theta_f = \\frac{\\pi s_{bpos}}{2.0 \\times 0.7} \\\\\n",
    "\\theta_f = \\frac{\\pi s_{bpos}}{1.4} \\\\\n",
    "R(s) = \\begin{cases}\n",
    "    \\sin(\\theta) & s_{bpos} \\geq 0.0 \\\\\n",
    "    \\sin(\\theta_f) & s_{bpos} < 0.0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The model's actions precipitated by this reward function exhibited more variety with respect to the side of the valley the car ventured in, but it unfortunately encouraged the model to attempt lingering at either side of the valley hills, regardless of how fast it was moving. Disregarding speed also disregarded momentum and, thus, did not yield a solution that achived victory. Sometimes, the model would favor the far side of the valley, completely disregarding the near side. After this attempt, it was obvious that the model needed to be discouraged from lingering at a specific height, reaping a constant reward, and from disregarding the near side of the valley in favor of the far side.\n",
    "\n",
    "### Attempt 6: Ranked Altitude Reward Based on Location and Speed\n",
    "The final attempt that, eventually, yielded a model which successfully reached the flag, started from the previous attempt's \"altitude\" reward approach. The prior approach was modified in two (2) ways:\n",
    "\n",
    "* The position reward in the far side of the valley would be \"muted\" by a fixed factor.\n",
    "* The total reward would be scaled by the car's speed.\n",
    "\n",
    "The selection of the \"muting factor\" for the far side was strictly based on empirical observations, and eventually the value $0.6$ yielded success. Also, the reward would be \"scaled\" by the car's speed (i.e., the car's velocity without regard to direction). The last addition aimed at disincentivizing the lingering behavior observed when using the last reward function. Basically, the model would be rewarded more as the car gained speed. Further, the speed reward in the near side of the valley (closest to the flag) would be increased by a factor of $2.0$, in order to further incentivize fast motion in the valley's near side.\n",
    "\n",
    "$$\n",
    "R(s,v) = \\begin{cases}\n",
    "    2.0 \\times \\sin(\\theta) \\times \\lvert v \\rvert & s_{bpos} \\geq 0.0 \\\\\n",
    "    0.6 \\times \\sin(\\theta_f) \\times \\lvert v \\rvert & s_{bpos} < 0.0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_reward(cart_position, cart_velocity, done, remaining_timesteps):\n",
    "    reward = cart_position + 0.5\n",
    "    if reward < 0.0:\n",
    "        reward = (math.fabs(reward) * math.pi) / 1.4\n",
    "        reward = math.sin(reward) * 0.6\n",
    "        reward *= math.fabs(cart_velocity)\n",
    "    else:\n",
    "        reward = (reward * math.pi) / 2.0\n",
    "        reward = math.sin(reward)\n",
    "        reward *= (2.0 * math.fabs(cart_velocity))\n",
    "\n",
    "    # Reached the flag? BONUS\n",
    "    if (remaining_timesteps > 0) and done:\n",
    "        reward = 1000.0\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "net = Net(name = 'net',\n",
    "          hidden_layer_size = hidden_layer_size,\n",
    "          learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Total Reward: 0.044052\n",
      "Test 2 Total Reward: 0.062883\n",
      "Test 3 Total Reward: 0.274793\n",
      "Test 4 Total Reward: 0.111460\n",
      "Test 5 Total Reward: 0.138190\n",
      "Test 6 Total Reward: 0.149540\n",
      "Test 7 Total Reward: 0.162236\n",
      "Test 8 Total Reward: 0.042849\n",
      "Test 9 Total Reward: 0.187215\n",
      "Test 10 Total Reward: 0.369068\n",
      "Test 11 Total Reward: 0.159081\n",
      "Test 12 Total Reward: 0.030991\n",
      "Test 13 Total Reward: 0.154917\n",
      "Test 14 Total Reward: 0.040828\n",
      "Test 15 Total Reward: 0.124358\n",
      "Test 16 Total Reward: 0.161737\n",
      "Test 17 Total Reward: 0.520044\n",
      "Test 18 Total Reward: 0.077175\n",
      "Test 19 Total Reward: 0.172716\n",
      "Test 20 Total Reward: 0.277317\n",
      "Test 21 Total Reward: 0.117083\n",
      "Test 22 Total Reward: 0.297293\n",
      "Test 23 Total Reward: 0.477915\n",
      "Test 24 Total Reward: 0.142265\n",
      "Test 25 Total Reward: 1.082097\n",
      "Test 26 Total Reward: 0.723146\n",
      "Test 27 Total Reward: 0.804612\n",
      "Test 28 Total Reward: 0.687000\n",
      "Test 29 Total Reward: 0.127395\n",
      "Test 30 Total Reward: 0.945647\n",
      "Test 31 Total Reward: 0.140894\n",
      "Test 32 Total Reward: 1.256090\n",
      "Test 33 Total Reward: 0.266965\n",
      "Test 34 Total Reward: 1.167581\n",
      "Test 35 Total Reward: 0.136303\n",
      "Test 36 Total Reward: 0.868650\n",
      "Test 37 Total Reward: 2.559748\n",
      "Test 38 Total Reward: 2.762884\n",
      "Test 39 Total Reward: 2.639319\n",
      "Test 40 Total Reward: 0.468864\n",
      "Test 41 Total Reward: 0.452681\n",
      "Test 42 Total Reward: 0.808667\n",
      "Test 43 Total Reward: 1002.146537\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import bisect\n",
    "import time\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_index = int(max_episodes * percentile / 100)\n",
    "    test_index = 0\n",
    "    while True:\n",
    "        total_reward_list = []\n",
    "        trajectory_list = []\n",
    "\n",
    "        for e in np.arange(max_episodes):\n",
    "            total_reward = 0.0\n",
    "            trajectory = []\n",
    "            state = env.reset()\n",
    "            for s in np.arange(max_steps):\n",
    "                action = net.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                reward = calculate_reward(next_state[0], next_state[1], done, max_steps - (s+1))\n",
    "                total_reward += reward\n",
    "                trajectory.append((state, action))\n",
    "                state = next_state\n",
    "                if done: break\n",
    "\n",
    "            index = bisect.bisect(total_reward_list, total_reward)\n",
    "            total_reward_list.insert(index, total_reward)\n",
    "            trajectory_list.insert(index, trajectory)\n",
    "\n",
    "        # keep the elite episodes, that is, throw out the bad ones \n",
    "        # train on state action pairs extracted from the elite episodes\n",
    "        # this code is not optimized, it can be cleaned up \n",
    "        state_action_pairs = []\n",
    "        for trajectory in trajectory_list[start_index:]:\n",
    "            for state_action_pair in trajectory:\n",
    "                state_action_pairs.append(state_action_pair)\n",
    "        # shuffle to avoid correlations between adjacent states\n",
    "        random.shuffle(state_action_pairs) \n",
    "        n = len(state_action_pairs)\n",
    "        batches = [state_action_pairs[k:k + batch_size] for k in np.arange(0, n, batch_size)]\n",
    "\n",
    "        for batch in batches:\n",
    "            net.train(batch)\n",
    "\n",
    "        # test agent\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        total_reward = 0.0\n",
    "        for s in np.arange(max_steps):\n",
    "            action = net.get_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward = calculate_reward(state[0], state[1], done, max_steps - (s+1))\n",
    "            total_reward += reward\n",
    "            env.render()\n",
    "            if done: break\n",
    "        env.close()\n",
    "        print(\"Test {:d} Total Reward: {:f}\".format(test_index + 1, total_reward))\n",
    "        if total_reward > 1000.0:\n",
    "            break\n",
    "        test_index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
